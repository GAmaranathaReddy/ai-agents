# LLM-Enhanced Agent

## LLM-Enhanced Agent Pattern

The LLM-Enhanced Agent pattern involves integrating a Large Language Model (LLM) into an agent's decision-making or response-generation process. Instead of relying solely on pre-programmed logic or rules, the agent leverages the LLM's capabilities (e.g., natural language understanding, generation, reasoning) to handle more complex tasks, understand nuanced user inputs, or provide more human-like and contextually aware responses.

The core idea is to use the LLM as a "cognitive core" or a specialized component that the main agent system can query. The agent prepares the input for the LLM, calls the LLM, and then processes the LLM's output to fit the overall task or conversation.

This pattern allows developers to build more sophisticated and flexible agents by combining the strengths of traditional software engineering with the power of modern LLMs.

## Implementation

This project demonstrates this pattern by connecting to a locally running Ollama instance.

1.  **`agent.py`**:
    *   Defines an `LLMEnhancedAgent` class.
    *   The `__init__` method initializes the agent and allows specifying a conceptual name for the LLM service it might interact with.
    *   The `__init__` method initializes the agent, allowing specification of an Ollama model (defaults to `"mistral"`).
    *   The `get_llm_response(user_input)` method now makes a direct API call to a locally running Ollama instance using the `ollama.chat()` method. It sends the user's input and retrieves the model's response. It includes basic error handling for common Ollama issues (e.g., service not running, model not pulled).
    *   The `process_request(user_input)` method orchestrates the interaction: it takes user input, calls `get_llm_response` to get the LLM's output, and then incorporates this into a final response string.

2.  **`main.py` (CLI)**:
    *   Provides a command-line interface to interact with the `LLMEnhancedAgent`.
    *   Instantiates the agent (which will use Ollama).
    *   Takes user input in a loop and prints the agent's Ollama-powered response.

3.  **`app_ui.py` (Streamlit UI)**:
    *   Provides a web-based interface using Streamlit.
    *   Allows users to enter queries and see responses generated by the agent via Ollama.

The "enhancement" by the LLM is now demonstrated through actual interaction with a local LLM, providing more dynamic and contextually relevant responses than the previous placeholder.

## Prerequisites for Running with Ollama

1.  **Install Ollama**:
    Ensure Ollama is installed and running on your system. Visit the [Ollama official website](https://ollama.com/) for download and installation instructions.

2.  **Pull an Ollama Model**:
    The agent defaults to using the `"mistral"` model. You need to pull this model using the Ollama CLI:
    ```bash
    ollama pull mistral
    ```
    If you wish to use a different model (e.g., `"llama2"`, `"nous-hermes2"`), you can modify the `model_name` when instantiating `LLMEnhancedAgent` in `agent.py`, `main.py`, or `app_ui.py`. Make sure to pull your chosen model, e.g., `ollama pull your_chosen_model_name`.

3.  **Install Python Dependencies**:
    The agent requires the `ollama` Python client library.
    ```bash
    pip install ollama
    ```
    If you are managing dependencies with Poetry for the main project, add `ollama` to your `pyproject.toml` and run `poetry install`.

## How to Run

### 1. Command-Line Interface (CLI)

1.  **Navigate to the agent's directory**:
    Open your terminal or command prompt.
    ```bash
    cd path/to/your/llm_enhanced_agent
    ```
    *(Ensure your environment has the `ollama` package installed and Ollama service is running with the required model pulled, as per Prerequisites.)*

2.  **Run the `main.py` script**:
    ```bash
    python main.py
    ```
    Or, if you have multiple Python versions:
    ```bash
    python3 main.py
    ```

3.  **Interact with the Agent via CLI**:
    The CLI will start, and you'll see a prompt like `You: `. Type your message and press Enter. The agent will respond, showing its simulated LLM-enhanced output.
    ```
    LLM-Enhanced Agent CLI
    ------------------------------
    Type 'exit' or 'quit' to stop.
    You: Hello there!
    Agent: Agent: Here's what the LLM (Ollama/mistral) came up with: Hello! It's nice to meet you. How can I help you today?
    You: exit
    Exiting agent CLI. Goodbye!
    ```
    *(The exact LLM response will vary.)*

### Web UI (Streamlit)

This agent also comes with a simple web-based user interface built with Streamlit.

1.  **Install Streamlit (if not already installed)**:
    Refer to the "Prerequisites" section above if you also need to install `ollama`. Streamlit can be installed via pip:
    ```bash
    pip install streamlit
    ```
    *(Ensure your environment has the `ollama` package installed and Ollama service is running with the required model pulled, as per Prerequisites.)*

2.  **Run the Streamlit App**:
    To run the web UI, navigate to the root directory of this repository (where the main `README.md` is) and use the following command in your terminal:
    ```bash
    streamlit run llm_enhanced_agent/app_ui.py
    ```
    This will typically open the application in your default web browser.

3.  **Interact with the Agent via Web**:
    *   The web page will display a title and a text input box.
    *   Type your query into the text box.
    *   Click the "Send to Agent" button.
    *   The agent's response will appear below the button.
